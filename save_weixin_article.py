#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¾®ä¿¡å…¬ä¼—å·æ–‡ç« ä¿å­˜å·¥å…·
å¯ä»¥å°†å¾®ä¿¡å…¬ä¼—å·æ–‡ç« ä¿å­˜åˆ°æœ¬åœ°ï¼ŒåŒ…æ‹¬æ–‡ç« å†…å®¹å’Œå›¾ç‰‡
è§£å†³è·¨åŸŸé—®é¢˜ï¼šé€šè¿‡æ¨¡æ‹Ÿæµè§ˆå™¨è¯·æ±‚å¤´æ¥ç»•è¿‡å¾®ä¿¡çš„é˜²ç›—é“¾æœºåˆ¶
"""

import os
import re
import sys
import time
import hashlib
import requests
import json
import feedparser
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse, unquote
import argparse


class WeixinArticleSaver:
    """å¾®ä¿¡å…¬ä¼—å·æ–‡ç« ä¿å­˜å™¨"""
    
    def __init__(self, output_dir="weixin_articles"):
        self.output_dir = output_dir
        self.session = requests.Session()
        
        # è®¾ç½®è¯·æ±‚å¤´ï¼Œæ¨¡æ‹Ÿæµè§ˆå™¨è®¿é—®ï¼Œè§£å†³è·¨åŸŸå’Œé˜²ç›—é“¾é—®é¢˜
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Cache-Control': 'max-age=0',
            'Upgrade-Insecure-Requests': '1',
        }
        
        # å›¾ç‰‡è¯·æ±‚å¤´ï¼Œå…³é”®æ˜¯è®¾ç½® Referer æ¥ç»•è¿‡é˜²ç›—é“¾
        self.image_headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'image/avif,image/webp,image/apng,image/svg+xml,image/*,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Referer': 'https://mp.weixin.qq.com/',  # å…³é”®ï¼šè®¾ç½® Referer ç»•è¿‡é˜²ç›—é“¾
        }
        
        self.session.headers.update(self.headers)
    
    def clean_filename(self, filename):
        """æ¸…ç†æ–‡ä»¶åï¼Œç§»é™¤éæ³•å­—ç¬¦"""
        # ç§»é™¤æˆ–æ›¿æ¢éæ³•å­—ç¬¦
        illegal_chars = r'[<>:"/\\|?*]'
        filename = re.sub(illegal_chars, '_', filename)
        # ç§»é™¤é¦–å°¾ç©ºæ ¼å’Œç‚¹
        filename = filename.strip(' .')
        # é™åˆ¶é•¿åº¦
        if len(filename) > 100:
            filename = filename[:100]
        return filename or 'untitled'
    
    def get_article_content(self, url):
        """è·å–æ–‡ç« å†…å®¹"""
        print(f"æ­£åœ¨è·å–æ–‡ç« : {url}")
        
        try:
            response = self.session.get(url, headers=self.headers, timeout=30)
            response.raise_for_status()
            response.encoding = 'utf-8'
            return response.text
        except requests.RequestException as e:
            print(f"è·å–æ–‡ç« å¤±è´¥: {e}")
            return None
    
    def extract_title(self, soup):
        """æå–æ–‡ç« æ ‡é¢˜"""
        # å°è¯•å¤šç§æ–¹å¼è·å–æ ‡é¢˜
        title = None
        
        # æ–¹å¼1: meta property="og:title"
        og_title = soup.find('meta', property='og:title')
        if og_title and og_title.get('content'):
            title = og_title['content']
        
        # æ–¹å¼2: id="activity-name"
        if not title:
            activity_name = soup.find(id='activity-name')
            if activity_name:
                title = activity_name.get_text(strip=True)
        
        # æ–¹å¼3: class="rich_media_title"
        if not title:
            rich_title = soup.find(class_='rich_media_title')
            if rich_title:
                title = rich_title.get_text(strip=True)
        
        # æ–¹å¼4: title æ ‡ç­¾
        if not title:
            title_tag = soup.find('title')
            if title_tag:
                title = title_tag.get_text(strip=True)
        
        return title or 'untitled'
    
    def extract_publish_time(self, soup):
        """æå–æ–‡ç« å‘å¸ƒæ—¶é—´ï¼Œè¿”å› Unix æ—¶é—´æˆ³å­—ç¬¦ä¸²"""
        # æ–¹å¼1: <em id="publish_time">2026å¹´1æœˆ4æ—¥ 18:08</em>
        publish_time_elem = soup.find(id='publish_time')
        if publish_time_elem:
            time_text = publish_time_elem.get_text(strip=True)
            # è§£æ "2026å¹´1æœˆ4æ—¥ 18:08" æ ¼å¼
            match = re.match(r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥\s*(\d{1,2}):(\d{2})', time_text)
            if match:
                from datetime import datetime
                year, month, day, hour, minute = map(int, match.groups())
                dt = datetime(year, month, day, hour, minute)
                timestamp = int(dt.timestamp())
                return str(timestamp)
        
        # æ–¹å¼2: var ct = "1234567890"
        # (å¦‚æœåŸå§‹HTMLä¸­å·²æœ‰ï¼Œç›´æ¥ç”¨)
        html_str = str(soup)
        ct_match = re.search(r'var\s+ct\s*=\s*"(\d+)"', html_str)
        if ct_match:
            return ct_match.group(1)
        
        return None
    
    def download_image(self, img_url, save_path, referer_url):
        """ä¸‹è½½å›¾ç‰‡ï¼Œå¤„ç†é˜²ç›—é“¾"""
        try:
            # å¤„ç†ç›¸å¯¹URL
            if img_url.startswith('//'):
                img_url = 'https:' + img_url
            elif img_url.startswith('/'):
                img_url = 'https://mp.weixin.qq.com' + img_url
            
            # å¤„ç†å¾®ä¿¡å›¾ç‰‡URLä¸­çš„ç‰¹æ®Šå‚æ•°
            # æœ‰äº›å›¾ç‰‡URLå¯èƒ½åŒ…å« wx_fmt å‚æ•°
            
            headers = self.image_headers.copy()
            headers['Referer'] = referer_url
            
            response = self.session.get(img_url, headers=headers, timeout=30, stream=True)
            response.raise_for_status()
            
            # æ£€æŸ¥å†…å®¹ç±»å‹
            content_type = response.headers.get('Content-Type', '')
            
            with open(save_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
            
            return True
        except Exception as e:
            print(f"  ä¸‹è½½å›¾ç‰‡å¤±è´¥ {img_url}: {e}")
            return False
    
    def process_images(self, soup, article_dir, article_url):
        """å¤„ç†æ–‡ç« ä¸­çš„æ‰€æœ‰å›¾ç‰‡"""
        images = soup.find_all('img')
        image_map = {}  # åŸå§‹URL -> æœ¬åœ°è·¯å¾„
        image_count = 0
        
        for img in images:
            # è·å–å›¾ç‰‡URLï¼Œå¾®ä¿¡æ–‡ç« å¯èƒ½ä½¿ç”¨ data-src æˆ– src
            img_url = img.get('data-src') or img.get('src')
            
            if not img_url:
                continue
            
            # è·³è¿‡ base64 å›¾ç‰‡å’Œç©ºç™½å›¾ç‰‡
            if img_url.startswith('data:') or 'spacer.gif' in img_url:
                continue
            
            # è·³è¿‡å·²å¤„ç†çš„å›¾ç‰‡
            if img_url in image_map:
                img['src'] = image_map[img_url]
                if img.get('data-src'):
                    del img['data-src']
                continue
            
            image_count += 1
            
            # ç¡®å®šå›¾ç‰‡æ‰©å±•å
            ext = '.jpg'  # é»˜è®¤æ‰©å±•å
            if 'wx_fmt=' in img_url:
                fmt_match = re.search(r'wx_fmt=(\w+)', img_url)
                if fmt_match:
                    fmt = fmt_match.group(1).lower()
                    if fmt in ['png', 'gif', 'jpeg', 'jpg', 'webp']:
                        ext = f'.{fmt}'
            elif '.' in urlparse(img_url).path:
                path_ext = os.path.splitext(urlparse(img_url).path)[1].lower()
                if path_ext in ['.png', '.gif', '.jpeg', '.jpg', '.webp', '.svg']:
                    ext = path_ext
            
            # ç”Ÿæˆæœ¬åœ°æ–‡ä»¶å
            local_filename = f"image_{image_count}{ext}"
            local_path = os.path.join(article_dir, local_filename)
            
            print(f"  ä¸‹è½½å›¾ç‰‡ {image_count}: {img_url[:80]}...")
            
            if self.download_image(img_url, local_path, article_url):
                # æ›´æ–°å›¾ç‰‡å¼•ç”¨ä¸ºæœ¬åœ°è·¯å¾„
                image_map[img_url] = local_filename
                img['src'] = local_filename
                if img.get('data-src'):
                    del img['data-src']
                print(f"    -> ä¿å­˜ä¸º {local_filename}")
            else:
                # ä¸‹è½½å¤±è´¥ï¼Œä¿ç•™åŸå§‹URL
                img['src'] = img_url
            
            # æ·»åŠ å»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡å¿«
            time.sleep(0.3)
        
        return image_count
    
    def clean_html(self, soup):
        """æ¸…ç†HTMLï¼Œç§»é™¤ä¸å¿…è¦çš„å…ƒç´ """
        # ç§»é™¤è„šæœ¬
        for script in soup.find_all('script'):
            script.decompose()
        
        # ç§»é™¤æ ·å¼è¡¨é“¾æ¥ï¼ˆä¿ç•™å†…è”æ ·å¼ï¼‰
        for link in soup.find_all('link', rel='stylesheet'):
            link.decompose()
        
        # ç§»é™¤ä¸€äº›å¾®ä¿¡ç‰¹æœ‰çš„éšè—å…ƒç´ 
        for elem in soup.find_all(class_=['qr_code_pc_outer', 'rich_media_tool', 'reward_area']):
            elem.decompose()
        
        # ç§»é™¤è¯„è®ºåŒº
        for elem in soup.find_all(id=['js_tpl_comment_area', 'js_comment_area']):
            elem.decompose()
        
        # ç§»é™¤éšè—æ ·å¼ï¼ˆå¾®ä¿¡ç”¨äºæ‡’åŠ è½½çš„æ ·å¼ï¼‰
        # æŸ¥æ‰¾å†…å®¹åŒºåŸŸå¹¶ç§»é™¤ visibility:hidden å’Œ opacity:0 æ ·å¼
        content_div = soup.find(id='js_content') or soup.find(class_='rich_media_content')
        if content_div:
            # ç§»é™¤éšè—æ ·å¼
            current_style = content_div.get('style', '')
            # ç§»é™¤ visibility: hidden å’Œ opacity: 0
            current_style = re.sub(r'visibility\s*:\s*hidden\s*;?\s*', '', current_style)
            current_style = re.sub(r'opacity\s*:\s*0\s*;?\s*', '', current_style)
            if current_style.strip():
                content_div['style'] = current_style.strip()
            elif content_div.has_attr('style'):
                del content_div['style']
        
        return soup
    
    def create_standalone_html(self, soup, title, publish_time=None):
        """åˆ›å»ºç‹¬ç«‹çš„HTMLæ–‡ä»¶ï¼Œä¿ç•™åŸå§‹å¸ƒå±€"""
        # è·å–æ–‡ç« ä¸»ä½“å†…å®¹
        content_div = soup.find(id='js_content') or soup.find(class_='rich_media_content')
        
        if not content_div:
            # å¦‚æœæ‰¾ä¸åˆ°ä¸»ä½“å†…å®¹ï¼Œä½¿ç”¨æ•´ä¸ªbody
            content_div = soup.find('body') or soup
        
        # æå–æ‰€æœ‰å†…è”æ ·å¼
        styles = []
        for style in soup.find_all('style'):
            styles.append(style.get_text())
        
        # è·å–å†…å®¹çš„HTMLå­—ç¬¦ä¸²
        content_html = str(content_div)
        
        # æ ¼å¼åŒ–å‘å¸ƒæ—¶é—´
        formatted_time = ""
        if publish_time:
            try:
                from datetime import datetime
                dt = datetime.fromtimestamp(int(publish_time))
                formatted_time = dt.strftime('%Yå¹´%#mæœˆ%#dæ—¥ %H:%M')
                # Windowsä¸Šä½¿ç”¨ %#m %#d å»é™¤å‰å¯¼é›¶ï¼ŒLinuxä¸Šä½¿ç”¨ %-m %-d
                # ä¸ºäº†è·¨å¹³å°å…¼å®¹ï¼Œè¿™é‡Œç®€å•å¤„ç†ä¸€ä¸‹
                if os.name != 'nt': 
                     formatted_time = dt.strftime('%Yå¹´%-mæœˆ%-dæ—¥ %H:%M')
            except Exception as e:
                print(f"æ—¶é—´æ ¼å¼åŒ–å¤±è´¥: {e}")

        # åˆ›å»ºæ–°çš„HTMLæ–‡æ¡£ï¼Œä¿ç•™å¾®ä¿¡åŸå§‹æ ·å¼
        html_template = f'''<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>{title}</title>
    <style>
        /* åŸºç¡€æ ·å¼ */  
        * {{
            box-sizing: border-box;
        }}
        html {{
            font-size: 16px;
        }}
        body {{
            font-family: -apple-system, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;
            line-height: 1.6;
            color: #333;
            background-color: #f5f5f5;
            margin: 0;
            padding: 0;
        }}
        .weixin-article-wrapper {{
            max-width: 677px;
            margin: 0 auto;
            background-color: #fff;
            padding: 20px;
            min-height: 100vh;
        }}
        .article-title {{
            font-size: 22px;
            font-weight: bold;
            margin-bottom: 20px;
            color: #333;
            text-align: center;
            line-height: 1.4;
            padding: 0 10px;
        }}
        .article-meta {{
            margin-bottom: 20px;
            text-align: center;
            color: rgba(0,0,0,0.3);
            font-size: 15px;
        }}
        /* å¾®ä¿¡æ–‡ç« å†…å®¹æ ·å¼ */
        #js_content, .rich_media_content {{
            overflow: hidden;
            font-size: 17px;
            word-wrap: break-word;
            -webkit-hyphens: auto;
            -ms-hyphens: auto;
            hyphens: auto;
            text-align: justify;
            position: relative;
        }}
        /* å›¾ç‰‡æ ·å¼ */
        #js_content img, .rich_media_content img {{
            max-width: 100% !important;
            height: auto !important;
            display: block;
            margin: 0 auto;
        }}
        /* section æ ·å¼ */
        section {{
            box-sizing: border-box;
        }}
        /* é“¾æ¥æ ·å¼ */
        a {{
            color: #576b95;
            text-decoration: none;
        }}
        a:hover {{
            text-decoration: underline;
        }}
        /* æ®µè½æ ·å¼ */
        p {{
            margin: 0;
            padding: 0;
        }}
        /* éšè—å¾®ä¿¡ç‰¹æœ‰ç»„ä»¶ */
        mp-common-profile,
        .mp_profile_iframe_wrp,
        mp-style-type {{
            display: none !important;
        }}
        /* åŸå§‹æ ·å¼ */
        {chr(10).join(styles)}
    </style>
    <script>var ct = "{publish_time or ''}";</script>
</head>
<body>
    <div class="weixin-article-wrapper">
        <h1 class="article-title">{title}</h1>
        <div class="article-meta">
            {f'<em id="publish_time" class="rich_media_meta rich_media_meta_text">{formatted_time}</em>' if formatted_time else ''}
        </div>
        <div class="article-content">
            {content_html}
        </div>
    </div>
</body>
</html>'''
        
        return html_template
    
    def save_article(self, url):
        """ä¿å­˜æ–‡ç« åˆ°æœ¬åœ°"""
        # è·å–æ–‡ç« å†…å®¹
        html_content = self.get_article_content(url)
        if not html_content:
            return False
        
        # è§£æHTML
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # æå–æ ‡é¢˜
        title = self.extract_title(soup)
        clean_title = self.clean_filename(title)
        
        print(f"æ–‡ç« æ ‡é¢˜: {title}")
        
        # åˆ›å»ºæ–‡ç« ç›®å½•
        article_dir = os.path.join(self.output_dir, clean_title)
        os.makedirs(article_dir, exist_ok=True)
        
        # æå–å‘å¸ƒæ—¶é—´ï¼ˆåœ¨æ¸…ç†HTMLä¹‹å‰ï¼‰
        publish_time = self.extract_publish_time(soup)
        if publish_time:
            print(f"å‘å¸ƒæ—¶é—´æˆ³: {publish_time}")
        
        # æ¸…ç†HTML
        soup = self.clean_html(soup)
        
        # å¤„ç†å›¾ç‰‡
        print("æ­£åœ¨ä¸‹è½½å›¾ç‰‡...")
        image_count = self.process_images(soup, article_dir, url)
        print(f"å…±ä¸‹è½½ {image_count} å¼ å›¾ç‰‡")
        
        # åˆ›å»ºç‹¬ç«‹HTML
        final_html = self.create_standalone_html(soup, title, publish_time)
        
        # ä¿å­˜HTMLæ–‡ä»¶
        html_path = os.path.join(article_dir, f"{clean_title}.html")
        with open(html_path, 'w', encoding='utf-8') as f:
            f.write(final_html)
        
        print(f"\næ–‡ç« å·²ä¿å­˜åˆ°: {article_dir}")
        print(f"HTMLæ–‡ä»¶: {html_path}")
        
        return True


class WeixinAutoMonitor:
    """å¾®ä¿¡å…¬ä¼—å·è‡ªåŠ¨ç›‘æ§å™¨"""
    
    def __init__(self, config_path="config.json", history_path="downloaded_history.json"):
        self.config_path = config_path
        self.history_path = history_path
        self.config = self.load_config()
        self.history = self.load_history()
        self.saver = WeixinArticleSaver(output_dir=self.config.get("output_dir", "weixin_articles"))
        
    def load_config(self):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            if os.path.exists(self.config_path):
                with open(self.config_path, 'r', encoding='utf-8') as f:
                    return json.load(f)
            return {}
        except Exception as e:
            print(f"åŠ è½½é…ç½®å¤±è´¥: {e}")
            return {}
            
    def load_history(self):
        """åŠ è½½å†å²è®°å½•"""
        try:
            if os.path.exists(self.history_path):
                with open(self.history_path, 'r', encoding='utf-8') as f:
                    return json.load(f)
            return {"downloaded_urls": []}
        except Exception as e:
            print(f"åŠ è½½å†å²è®°å½•å¤±è´¥: {e}")
            return {"downloaded_urls": []}
            
    def save_history(self):
        """ä¿å­˜å†å²è®°å½•"""
        try:
            with open(self.history_path, 'w', encoding='utf-8') as f:
                json.dump(self.history, f, ensure_ascii=False, indent=4)
        except Exception as e:
            print(f"ä¿å­˜å†å²è®°å½•å¤±è´¥: {e}")
            
    def check_and_download(self):
        """æ£€æŸ¥æ›´æ–°å¹¶ä¸‹è½½æ–°æ–‡ç« """
        rsshub_base = self.config.get("rsshub_base_url", "http://localhost:1200")
        accounts = self.config.get("accounts", [])
        
        if not accounts:
            print("âŒ é…ç½®æ–‡ä»¶ä¸­æœªæ‰¾åˆ°å…¬ä¼—å·åˆ—è¡¨")
            return
            
        for account in accounts:
            name = account.get("name", "Unknown")
            biz = account.get("biz")
            # æ”¯æŒç›´æ¥é…ç½® RSS URL
            rss_url = account.get("rss_url")
            
            if not rss_url:
                if not biz:
                    print(f"âš ï¸ å¿½ç•¥å…¬ä¼—å· {name}: ç¼ºå°‘ biz æˆ– rss_url")
                    continue
                rss_url = f"{rsshub_base}/wechat/mp/msgalist/{biz}"
                
            print(f"\n[Monitor] æ­£åœ¨æ£€æŸ¥å…¬ä¼—å·: {name}")
            print(f"  æºåœ°å€: {rss_url}")
            
            try:
                feed = feedparser.parse(rss_url)
                
                if hasattr(feed, 'status'):
                    print(f"  RSS å“åº”çŠ¶æ€ç : {feed.status}")
                
                if not feed.entries:
                    print(f"  âŒ æœªå‘ç°æ–‡ç« æˆ–æŠ“å–å¤±è´¥ã€‚")
                    if hasattr(feed, 'status') and feed.status == 503:
                        print("  ğŸ’¡ æç¤º: RSSHub è¿”å› 503ï¼Œé€šå¸¸æ˜¯ç”±äºè¢«å¾®ä¿¡å°é”æˆ–è·¯ç”±å·²å¤±æ•ˆã€‚")
                    continue
                    
                new_articles_count = 0
                for entry in feed.entries:
                    url = entry.link
                    clean_url = url.split('&')[0] if 'mp.weixin.qq.com' in url else url
                    
                    if clean_url in self.history["downloaded_urls"]:
                        continue
                        
                    # æ ‡é¢˜å…³é”®è¯è¿‡æ»¤
                    keywords = account.get("keywords", [])
                    if keywords:
                        match = any(kw in entry.title for kw in keywords)
                        if not match:
                            # å¦‚æœæ˜¯åˆå§‹åŒ–æ¨¡å¼ï¼Œæˆ‘ä»¬ä¹ŸæŠŠä¸åŒ¹é…çš„ä¹Ÿè®°å½•ä¸ºâ€œå·²é˜…â€ï¼Œé¿å…ä»¥åå¤šä½™æ£€æŸ¥
                            if self.config.get("init_only", False):
                                self.history["downloaded_urls"].append(clean_url)
                            continue

                    print(f"  ğŸ¯ å‘ç°åŒ¹é…æ–°æ–‡ç« : {entry.title}")
                    
                    # å¦‚æœæ˜¯åˆå§‹åŒ–æ¨¡å¼ï¼Œä¸æ‰§è¡Œä¸‹è½½ï¼Œä»…è®°å½•å†å²
                    if self.config.get("init_only", False):
                        print(f"    [Init] å·²æ ‡è®°ä¸ºå·²ä¸‹è½½ (ä¸æ‰§è¡Œä¸‹è½½)")
                        self.history["downloaded_urls"].append(clean_url)
                        new_articles_count += 1
                        continue

                    if self.saver.save_article(url):
                        self.history["downloaded_urls"].append(clean_url)
                        new_articles_count += 1
                        if new_articles_count >= 5:
                            break
                    time.sleep(2)
                
                if new_articles_count > 0:
                    status_text = "æ ‡è®°" if self.config.get("init_only") else "ä¸‹è½½"
                    print(f"  âœ… å…¬ä¼—å· {name} å¤„ç†å®Œæˆï¼Œ{status_text}äº† {new_articles_count} ç¯‡æ–‡ç« ")
                    self.save_history()
                else:
                    print(f"  â˜• å…¬ä¼—å· {name} æ— åŒ¹é…æ›´æ–°")
                    
            except Exception as e:
                print(f"  ğŸ’¥ å¤„ç†å…¬ä¼—å· {name} æ—¶å‡ºé”™: {e}")


def main():
    parser = argparse.ArgumentParser(
        description='å¾®ä¿¡å…¬ä¼—å·æ–‡ç« ä¿å­˜å·¥å…· - å°†å¾®ä¿¡å…¬ä¼—å·æ–‡ç« ä¿å­˜åˆ°æœ¬åœ°',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog='''
ç¤ºä¾‹:
  python save_weixin_article.py https://mp.weixin.qq.com/s/xxxxx
  python save_weixin_article.py --auto
  python save_weixin_article.py -c config.json --auto
        '''
    )
    parser.add_argument('url', nargs='?', help='å¾®ä¿¡å…¬ä¼—å·æ–‡ç« URL')
    parser.add_argument('-o', '--output', default='weixin_articles', 
                        help='è¾“å‡ºç›®å½• (é»˜è®¤: weixin_articles)')
    parser.add_argument('--auto', action='store_true', help='å¼€å¯è‡ªåŠ¨ç›‘æ§æ¨¡å¼')
    parser.add_argument('-c', '--config', default='config.json', help='é…ç½®æ–‡ä»¶è·¯å¾„ (ä»…è‡ªåŠ¨æ¨¡å¼ä½¿ç”¨)')
    parser.add_argument('--history', default='downloaded_history.json', help='å†å²è®°å½•æ–‡ä»¶è·¯å¾„')
    
    args = parser.parse_args()
    
    if args.auto:
        print("[Mode] å¼€å¯è‡ªåŠ¨ç›‘æ§ä¸‹è½½æ¨¡å¼")
        monitor = WeixinAutoMonitor(config_path=args.config, history_path=args.history)
        monitor.check_and_download()
    elif args.url:
        # éªŒè¯URL
        if 'mp.weixin.qq.com' not in args.url:
            print("è­¦å‘Š: è¿™å¯èƒ½ä¸æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„å¾®ä¿¡å…¬ä¼—å·æ–‡ç« é“¾æ¥")
            print("å¾®ä¿¡å…¬ä¼—å·æ–‡ç« é“¾æ¥é€šå¸¸ä»¥ https://mp.weixin.qq.com/s/ å¼€å¤´")
        
        # åˆ›å»ºä¿å­˜å™¨å¹¶ä¿å­˜æ–‡ç« 
        saver = WeixinArticleSaver(output_dir=args.output)
        success = saver.save_article(args.url)
        
        if success:
            print("\n[OK] æ–‡ç« ä¿å­˜æˆåŠŸ!")
        else:
            print("\n[FAIL] æ–‡ç« ä¿å­˜å¤±è´¥!")
            sys.exit(1)
    else:
        parser.print_help()


if __name__ == '__main__':
    main()